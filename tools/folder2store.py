import os
import numpy as np
import pandas as pd
import chromadb
import nltk
from collections import Counter
from soynlp.noun import LRNounExtractor_v2
from sentence_transformers import SentenceTransformer
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
import traceback
from ollama import Client
import ast

def get_embedding(text: str) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ SBERTë¥¼ ì´ìš©í•´ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")  # âœ… í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ëª¨ë¸ ì •ì˜
    return embedding_model.encode(text).tolist()  # ChromaDBëŠ” list í˜•íƒœë§Œ í—ˆìš©

def keywords_on_text(noun_list: list, lecture_name: str, client: Client, model_name: str = "llama3.1:8b") -> list:
    """
    Ollama LLMì„ í™œìš©í•˜ì—¬ ë³µí•© ëª…ì‚¬ ì¤‘ ë…ë¦½ì ìœ¼ë¡œ ê°•ì˜ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ë§Œ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
    """
    SYS_PROMPT = (
        "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ê°•ì˜ëª…ê³¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬,"
        "ë…ë¦½ì ìœ¼ë¡œ ê°•ì˜ ë‚´ìš©ì„ ëª…í™•íˆ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œë§Œì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤.\n\n"
        "ê·œì¹™:\n"
        "1) í‚¤ì›Œë“œëŠ” ê°•ì˜ëª…ê³¼ ê´€ë ¨ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n"
        "2) í‚¤ì›Œë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ ë³´ì•˜ì„ ë•Œë„ ì˜ë¯¸ê°€ ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "   (ì˜ˆ: 'ë¬¸ì œ í’€ì´'ëŠ” ëª¨í˜¸í•˜ì§€ë§Œ 'ìˆ˜í•™ ë¬¸ì œ í’€ì´'ëŠ” ëª…í™•í•¨)\n"
        "3) ë°˜ë“œì‹œ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”,\n"
        "   ë°˜ë“œì‹œ ì‘ë‹µì— ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "4) ì¤‘ë³µëœ í‚¤ì›Œë“œëŠ” ì œê±°í•˜ê³ , ì¤‘ìš”í•œ í‚¤ì›Œë“œë§Œ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n\n"
        "ì¶œë ¥ ì˜ˆì‹œ:\n"
        "['íšŒê·€ ë¶„ì„', 'í™•ë¥  í†µê³„', 'ë°ì´í„° ë§ˆì´ë‹']"
    )

    USER_PROMPT = f"ê°•ì˜ëª…: {lecture_name}\në³µí•© ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸: {noun_list}\nì¶œë ¥: "
    response = client.generate(
        model=model_name,
        system=SYS_PROMPT,
        prompt=USER_PROMPT
    )
    
    # ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    raw_result = response.response
    try:
        keywords_list = ast.literal_eval(raw_result)
        return keywords_list
    except (SyntaxError, ValueError):
        print(f"âš ï¸ LLM ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨! ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•¨.")
        return []


def excel2df(file_path: str, client: Client) -> tuple: 
    '''
    ì—‘ì…€ íŒŒì¼ì—ì„œ ê°•ì˜ëª…, ê°•ì˜ ê°œìš”, ìš”êµ¬ ì§€ì‹, ê°•ì˜ ê³„íšì„ ì¶”ì¶œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
    '''
    df = pd.DataFrame(pd.read_excel(file_path, header=None))
    name, abstract, required_knowledge, target_knowledge = '', '', '', ''
    plan = []

    # ê°•ì˜ ê°œìš” ì¶”ì¶œ
    abstract_r_index = df.iloc[:, 0].str.contains('ê°•ì˜ê°œìš”', na=False).idxmax()
    abstract = df.iloc[abstract_r_index + 1, 0]
    
    # ê°•ì˜ëª… ì¶”ì¶œ
    name_r_index = df.iloc[:, 18].str.contains('êµê³¼ëª©ëª…', na=False).idxmax()
    name = df.iloc[name_r_index, 24]

    # ì„ ìˆ˜ ê³¼ëª© ì¶”ì¶œ
    required_knowledge_r_index = df.iloc[:, 0].str.contains('ì„ ìˆ˜ê³¼ëª©', na=False).idxmax()
    required_knowledge = df.iloc[required_knowledge_r_index + 1, 0]
    required_knowledge = 'ì—†ìŒ' if pd.isna(required_knowledge) else required_knowledge
    
    # ê°•ì˜ ëª©í‘œ ì¶”ì¶œ
    target_knowledge_r_index = df.iloc[:, 0].str.contains('ê°•ì˜ëª©í‘œ', na=False).idxmax()
    target_knowledge = df.iloc[target_knowledge_r_index + 1, 0]

    # ê°•ì˜ ê³„íš ì¶”ì¶œ
    plan_startpoint_r_index = df.iloc[:, 0].str.contains('ì£¼ë³„', na=False).idxmax()
    for _ in range(plan_startpoint_r_index+1, plan_startpoint_r_index+58, 4):
        plan.append({
            'theme': df.iloc[_ , 10] if not pd.isna(df.iloc[_, 10]) else 'ì—†ìŒ',
            'goal': df.iloc[_ + 1, 10] if not pd.isna(df.iloc[_+1, 10]) else 'ì—†ìŒ',
            'keyword': df.iloc[_ + 2, 10] if not pd.isna(df.iloc[_+2, 10]) else 'ì—†ìŒ',
            'approach': df.iloc[_+3, 10] if not pd.isna(df.iloc[_+3, 10]) else 'ì—†ìŒ'
        })

    # DataFrame ìƒì„±
    plan_df = pd.DataFrame(plan)
    plan_df['name'] = name
    plan_df['abstract'] = abstract
    plan_df['required_knowledge'] = required_knowledge
    plan_df['target_knowledge'] = target_knowledge

    # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìœ¼ê¸°
    text_data = []
    for col in plan_df.columns:
        for text in plan_df[col].dropna():  # NaN ê°’ ì œì™¸
            if isinstance(text, str):  # ë¬¸ìì—´ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
                sentences = nltk.sent_tokenize(text)  # ë¬¸ì¥ ë¶„ë¦¬
                text_data.extend(sentences)

    # ë³µí•© ëª…ì‚¬ ì¶”ì¶œ
    noun_extractor = LRNounExtractor_v2()
    nouns = noun_extractor.train_extract(text_data)
    compound_nouns = [noun for noun in nouns.keys() if len(noun) > 1]

    # âœ… LLMì„ ì‚¬ìš©í•˜ì—¬ í•„í„°ë§
    filtered_nouns = keywords_on_text(compound_nouns, name, client)

    # ëª…ì‚¬ ë¹ˆë„ìˆ˜ ê³„ì‚°
    noun_counter = Counter()
    for sentence in text_data:
        for noun in filtered_nouns:
            if noun in sentence:
                noun_counter[noun] += 1

    # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ëª…ì‚¬ ë° ë¹ˆë„ìˆ˜)
    noun_freq_df = pd.DataFrame(noun_counter.items(), columns=['noun', 'frequency']).sort_values(by='frequency', ascending=False)

    return name, noun_freq_df

def noun_exists_in_db(noun: str) -> bool:
    """íŠ¹ì • ëª…ì‚¬ê°€ ì´ë¯¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    results = noun_vector_db.get(where={"noun": noun}, include=["embeddings", "metadatas"])
    return len(results["ids"]) > 0  # ëª…ì‚¬ê°€ ì¡´ì¬í•˜ë©´ True ë°˜í™˜

def store_noun_embeddings(noun_df):
    """ë³µí•© ëª…ì‚¬ ì„ë² ë”©ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€)"""
    for _, row in noun_df.iterrows():
        noun = row["noun"]
        
        # âœ… ì¤‘ë³µ í™•ì¸
        existing_data = noun_vector_db.get(include=["embeddings"], where={"noun": noun})
        if existing_data["ids"]:
            print(f"â© ëª…ì‚¬ '{noun}' ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì¶”ê°€í•˜ì§€ ì•ŠìŒ.")
            continue

        # âœ… ë²¡í„° ìƒì„±
        vector = get_embedding(noun) # NumPy ë°°ì—´ë¡œ ë³€í™˜

        # âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        try:
            noun_vector_db.add(
                ids=[noun], embeddings=[vector],  # âœ… ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì €ì¥
                metadatas=[{"noun": noun, "frequency": row["frequency"]}]
            )

            # âœ… ì €ì¥ í›„ í™•ì¸
            check_data = noun_vector_db.get(include=["embeddings"], where={"noun": noun})
            if not check_data["ids"]:
                print(f"âš ï¸ ëª…ì‚¬ '{noun}' ì €ì¥ ì‹¤íŒ¨! ChromaDBì— ì €ì¥ë˜ì§€ ì•ŠìŒ.")
            else:
                print(f"âœ… ëª…ì‚¬ '{noun}' ì €ì¥ ì™„ë£Œ! ë²¡í„° í¬ê¸°: {len(vector)}")

        except Exception as e:
            print(f"âš ï¸ ëª…ì‚¬ '{noun}' ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def lecture_exists_in_db(lecture_name: str) -> bool:
    """ê°•ì˜ëª…ì´ ì´ë¯¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    results = lecture_vector_db.get(where={"lecture_name": lecture_name}, include=["embeddings", "metadatas"])
    return len(results["ids"]) > 0  # ê°•ì˜ëª…ì´ ì¡´ì¬í•˜ë©´ True ë°˜í™˜

def store_lecture_embeddings(lecture_name, noun_df):
    """ê°•ì˜ í‰ê·  ì„ë² ë”©ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    
    # âœ… ëª¨ë“  ëª…ì‚¬ì˜ ì„ë² ë”©ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜
    embeddings = [get_embedding(noun) for noun in noun_df["noun"]]
    
    # âœ… í‰ê·  ë²¡í„° ê³„ì‚°
    lecture_vector = np.mean(embeddings, axis=0)  # âœ… NumPy ë°°ì—´ í˜•íƒœë¡œ ìœ ì§€

    try: 
        lecture_vector_db.add(
            ids=[lecture_name],
            embeddings=[lecture_vector],  # âœ… ë¦¬ìŠ¤íŠ¸ ë³€í™˜ í›„ ì €ì¥
            metadatas=[{"lecture_name": lecture_name}]
        )

        # âœ… ì €ì¥ í›„ í™•ì¸ (include=["embeddings"] ì¶”ê°€)
        check_data = lecture_vector_db.get(where={"lecture_name": lecture_name}, include=["embeddings", "metadatas"])
        
        if not check_data["ids"]:
            print(f"âš ï¸ ê°•ì˜ '{lecture_name}' ì €ì¥ ì‹¤íŒ¨! ChromaDBì— ì €ì¥ë˜ì§€ ì•ŠìŒ.")
        else:
            print(f"âœ… ê°•ì˜ '{lecture_name}' í‰ê·  ë²¡í„° ì €ì¥ ì™„ë£Œ! ë²¡í„° í¬ê¸°: {len(lecture_vector)}")

    except Exception as e:
        print(f"âš ï¸ ê°•ì˜ '{lecture_name}' ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")



def process_all_excel_files(folder_path, client: Client):
    """í´ë” ë‚´ ëª¨ë“  ì—‘ì…€ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³ , ì¤‘ë³µ ê°•ì˜ëª…ì„ ì œì™¸í•œ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì €ì¥"""
    nltk.download('punkt_tab')

    for file in os.listdir(folder_path):
        if file.endswith(".xlsx"):
            file_path = os.path.join(folder_path, file)
            
            try:
                lecture_name, noun_df = excel2df(file_path, client)
                
                if lecture_exists_in_db(lecture_name):
                    print(f"âœ… ê°•ì˜ '{lecture_name}'ëŠ” ì´ë¯¸ ì¡´ì¬í•¨, ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                    continue

                print(f"ğŸ“Œ ê°•ì˜ '{lecture_name}' ì²˜ë¦¬ ì¤‘...")

                # âœ… ì¤‘ë³µ ë°©ì§€í•˜ì—¬ ëª…ì‚¬ ë²¡í„° DB ì €ì¥
                store_noun_embeddings(noun_df)

                # âœ… ê°•ì˜ í‰ê·  ë²¡í„° DB ì €ì¥
                store_lecture_embeddings(lecture_name, noun_df)

                print(f"âœ… ê°•ì˜ '{lecture_name}' ë²¡í„° ì €ì¥ ì™„ë£Œ!\n")

            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ {file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                traceback.print_exc()  # âœ… ì˜¤ë¥˜ ì›ì¸ ì¶œë ¥


if __name__ == '__main__':
    
    # âœ… 1. ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    chroma_client = chromadb.PersistentClient(path="chroma_db")  

    # âœ… 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì»¬ë ‰ì…˜ ìƒì„±
    noun_vector_db = chroma_client.get_or_create_collection("noun_embeddings")   # ë³µí•© ëª…ì‚¬ ë²¡í„° DB
    lecture_vector_db = chroma_client.get_or_create_collection("lecture_embeddings")  # ê°•ì˜ í‰ê·  ë²¡í„° DB

    folder_path = "D:/CDM/Workspace/AcaTuning/user_input/ver2"
    
    # âœ… Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„± í›„ ì „ë‹¬
    ollama_client = Client()
    process_all_excel_files(folder_path, ollama_client)
    